import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import warnings
warnings.filterwarnings('ignore')
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
import pickle
import time
import joblib
from sklearn.model_selection import train_test_split, GridSearchCV
from catboost import CatBoostClassifier
from sklearn.metrics import f1_score
from sklearn.cluster import KMeans
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import RobustScaler
import dash
from dash import dcc, html
from dash.dependencies import Input, Output
import os
import joblib

st.set_page_config(
    page_title="ê±´ì„¤ ê¸°ê³„ ì˜¤ì¼ ìƒíƒœ ì§„ë‹¨",
    page_icon="ğŸ”§",
)

# Proba ë° ì˜ˆì¸¡ê²°ê³¼ í‘œ ìƒì„±
data = {
    "Proba": ["~0.40", "0.40~0.45", "0.45~0.50", "0.50~"],
    "ì˜ˆì¸¡ê²°ê³¼": ["ğŸŸ¢ ì •ìƒ", "ğŸŸ¡ ì£¼ì˜", "ğŸŸ  ê²½ê³ ", "ğŸ”´ ìœ„í—˜"]
}
tabel_side = pd.DataFrame(data)

# ì‚¬ì´ë“œë°”ì— í‘œ ì‚½ì…
st.sidebar.write("### ğŸš¦ì˜ˆì¸¡ ê²°ê³¼ì— ë”°ë¥¸ ìƒíƒœ")
st.sidebar.write(tabel_side.to_markdown(index=False), unsafe_allow_html=True)


st.title("âš™ï¸ì»´í¬ë„ŒíŠ¸ ë³„ ê±´ì„¤ ê¸°ê³„ ì˜¤ì¼ ìƒíƒœ ì§„ë‹¨")

############################ Component 1 ############################
# ë°ì´í„° ë¡œë“œ
st.subheader("ğŸ›¢ï¸COMPONENT1 ëª¨ë¸ë§ ê²°ê³¼")
df = pd.read_csv("./data/casting.csv")

# ì»´í¬ë„ŒíŠ¸ 1 ë¶„ë¦¬
df_1 = df[df['COMPONENT_ARBITRARY'] == 'COMPONENT1']

# ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
df_1 = df_1.drop(columns=['ID', 'COMPONENT_ARBITRARY', 'YEAR', 'SAMPLE_TRANSFER_DAY', 'U100', 'U75', 'U50', 'U25', 'U20', 'U14', 'U6', 'U4'])

# ê²°ì¸¡ì¹˜ ëŒ€ì¹˜
df_1['CD'] = df_1['CD'].fillna(df_1['CD'].mode()[0])
df_1['K'] = df_1['K'].fillna(df_1['K'].mean())

# ë…ë¦½ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ ë¶„í• 
y = df_1['Y_LABEL']
X = df_1.drop(columns=['Y_LABEL'])

# ë¡œë²„ìŠ¤íŠ¸ ìŠ¤ì¼€ì¼ë§
scaler = RobustScaler()
X = scaler.fit_transform(X)

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 
X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.12, random_state=42)

# CatBoost ëª¨ë¸ ì„¤ì • ë° í•™ìŠµ
cat_1 = CatBoostClassifier(
    iterations=140,
    learning_rate=0.17,
    depth=5,
    l2_leaf_reg=3,
    border_count=30,
    loss_function='Logloss',
    eval_metric='F1',
    verbose=10
)
cat_1.fit(X_tr, y_tr)

# ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
save_dir = 'models'
os.makedirs(save_dir, exist_ok=True)  # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±

# ëª¨ë¸ ì €ì¥
joblib.dump(cat_1, os.path.join(save_dir, 'cat_1_model.joblib'))

# í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ëŒ€í•´ ì˜ˆì¸¡
y_pred = cat_1.predict(X_val)

# F1 ìŠ¤ì½”ì–´ ê³„ì‚°
f1 = f1_score(y_val, y_pred)
print(f"F1 Score: {f1:.2f}")

# ë³€ìˆ˜ ì¤‘ìš”ë„ í™•ì¸
feature_importance = cat_1.get_feature_importance()
important_features = np.argsort(feature_importance)[-3:]

print(f"ê°€ì¥ ì¤‘ìš”í•œ ë³€ìˆ˜: {df_1.columns[important_features]}")

# ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜
def predict_fault(input_values, selected_features):
    data = np.zeros((1, X.shape[1]))
    for i, feature in enumerate(selected_features):
        data[0, list(df_1.columns).index(feature)] = input_values[i]
    data_scaled = scaler.transform(data)
    prediction = cat_1.predict_proba(data_scaled)[:, 1]
    if prediction[0] < 0.40:
        return f"ğŸŸ¢:ì •ìƒ: {prediction[0]:.2f} í™•ë¥ ë¡œ ë¶ˆëŸ‰ì…ë‹ˆë‹¤.ê³„ì† ì‚¬ìš©í•˜ì„¸ìš”."
    elif 0.40 <= prediction[0] < 0.45:
        return f"ğŸŸ¡ì£¼ì˜: {prediction[0]:.2f} í™•ë¥ ë¡œ ë¶ˆëŸ‰ì…ë‹ˆë‹¤.ì‚¬ìš©ì— ì£¼ì˜í•˜ì„¸ìš”."
    elif 0.45 <= prediction[0] < 0.50:
        return f"ğŸŸ ê²½ê³ : {prediction[0]:.2f} í™•ë¥ ë¡œ ë¶ˆëŸ‰ì…ë‹ˆë‹¤.ë§ˆìŒì˜ ì¤€ë¹„ê°€ í•„ìš”í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤."
    else:
        return f"ğŸ”´ìœ„í—˜: {prediction[0]:.2f} í™•ë¥ ë¡œ ë¶ˆëŸ‰ì…ë‹ˆë‹¤.ë‹¹ì¥ êµì²´í•˜ì„¸ìš”."


# ëª¨ë¸ í•™ìŠµ ë²„íŠ¼
if st.button("ëª¨ë¸ í•™ìŠµ", key="train_model"):
    cat_1.fit(X_tr, y_tr)
    st.write("ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# ë³€ìˆ˜ ì„ íƒ ë° ìŠ¬ë¼ì´ë” ì„¤ì •
features = list(df_1.columns)
selected_features = st.multiselect("ë³€ìˆ˜ ì„ íƒ", features)

# ì„ íƒí•œ ë³€ìˆ˜ë“¤ì— ëŒ€í•´ ìŠ¬ë¼ì´ë” ì„¤ì •
input_data = []
for feature in selected_features:
    min_val = float(df_1[feature].min())
    max_val = float(df_1[feature].max())
    value = st.slider(f"{feature} ê°’ì„ ì…ë ¥í•˜ì„¸ìš”:", min_value=min_val, max_value=max_val, value=(min_val + max_val) / 2)
    input_data.append(value)

# ì…ë ¥ ë°ì´í„°ë¡œ ì˜ˆì¸¡ ì‹¤í–‰
if st.button("ì˜ˆì¸¡ ì‹¤í–‰", key="predict"):
    if 'cat_1' in locals():
        input_scaled = np.array(input_data).reshape(1, -1)
        prediction_result = predict_fault(input_scaled[0], selected_features)
        st.write(prediction_result)
    else:
        st.write("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'ëª¨ë¸ í•™ìŠµ' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”.")
        
        
        
st.divider()


############################ Component 2 ############################
# ë°ì´í„° ë¡œë“œ
st.subheader("ğŸ›¢ï¸COMPONENT2 ëª¨ë¸ë§ ê²°ê³¼")
df = pd.read_csv("./data/casting.csv")

# ì»´í¬ë„ŒíŠ¸ 2 ë¶„ë¦¬
df_2 = df[df['COMPONENT_ARBITRARY'] == 'COMPONENT2']

# ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
df_2 = df_2.drop(columns=['ID', 'COMPONENT_ARBITRARY', 'YEAR', 'SAMPLE_TRANSFER_DAY', 'FH2O', 'FNOX', 'FOPTIMETHGLY', 'FOXID', 'FSO4', 'FTBN', 'FUEL', 'SOOTPERCENTAGE', 'U4', 'U6', 'U14', 'U20', 'U25', 'U50', 'U75', 'U100', 'V100'])

# ê²°ì¸¡ì¹˜ ëŒ€ì¹˜
df_2['CD'] = df_2['CD'].fillna(df_2['CD'].mode()[0])
df_2['K'] = df_2['K'].fillna(df['K'].mean())

# ë…ë¦½ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ ë¶„í• 
y = df_2['Y_LABEL']
X = df_2.drop(columns=['Y_LABEL'])

# ë¡œë²„ìŠ¤íŠ¸ ìŠ¤ì¼€ì¼ë§
scaler = RobustScaler()
X = scaler.fit_transform(X)

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 
X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)

# CatBoost ëª¨ë¸ ì„¤ì • ë° í•™ìŠµ
cat_2 = CatBoostClassifier(
    iterations=120,
    learning_rate=0.7,
    depth=4,
    l2_leaf_reg=4,
    border_count=150,
    loss_function='Logloss',
    eval_metric='F1',
    verbose=10
)
cat_2.fit(X_tr, y_tr)

# ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
save_dir = 'models'
os.makedirs(save_dir, exist_ok=True)  # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±

# ëª¨ë¸ ì €ì¥
joblib.dump(cat_2, os.path.join(save_dir, 'cat_2_model.joblib'))

# í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ëŒ€í•´ ì˜ˆì¸¡
y_pred = cat_2.predict(X_val)

# F1 ìŠ¤ì½”ì–´ ê³„ì‚°
f1 = f1_score(y_val, y_pred)
print(f"F1 Score: {f1:.2f}")

# ë³€ìˆ˜ ì¤‘ìš”ë„ í™•ì¸
feature_importance = cat_2.get_feature_importance()
important_features = np.argsort(feature_importance)[-3:]

print(f"ê°€ì¥ ì¤‘ìš”í•œ ë³€ìˆ˜: {df_2.columns[important_features]}")

# ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜
def predict_fault(input_values, selected_features):
    data = np.zeros((1, X.shape[1]))
    for i, feature in enumerate(selected_features):
        data[0, list(df_2.columns).index(feature)] = input_values[i]
    data_scaled = scaler.transform(data)
    prediction = cat_2.predict_proba(data_scaled)[:, 1]
    if prediction[0] < 0.40:
        return f"ğŸŸ¢:ì •ìƒ: {prediction[0]:.2f} í™•ë¥ ë¡œ ë¶ˆëŸ‰ì…ë‹ˆë‹¤. ê³„ì† ì‚¬ìš©í•˜ì„¸ìš”."
    elif 0.40 <= prediction[0] < 0.45:
        return f"ğŸŸ¡ì£¼ì˜: {prediction[0]:.2f} í™•ë¥ ë¡œ ë¶ˆëŸ‰ì…ë‹ˆë‹¤. ì‚¬ìš©ì— ì£¼ì˜í•˜ì„¸ìš”."
    elif 0.45 <= prediction[0] < 0.50:
        return f"ğŸŸ ê²½ê³ : {prediction[0]:.2f} í™•ë¥ ë¡œ ë¶ˆëŸ‰ì…ë‹ˆë‹¤. ë§ˆìŒì˜ ì¤€ë¹„ê°€ í•„ìš”í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤."
    else:
        return f"ğŸ”´ìœ„í—˜: {prediction[0]:.2f} í™•ë¥ ë¡œ ë¶ˆëŸ‰ì…ë‹ˆë‹¤. ë‹¹ì¥ êµì²´í•˜ì„¸ìš”."

# ëª¨ë¸ í•™ìŠµ ë²„íŠ¼
if st.button("ğŸ›«ëª¨ë¸ í•™ìŠµ", key="train_model_button"):
    cat_2.fit(X_tr, y_tr)
    st.write("ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# ë³€ìˆ˜ ì„ íƒ ë° ìŠ¬ë¼ì´ë” ì„¤ì •
features = list(df_2.columns)
feature_selector_key = "feature_selector_" + str(np.random.randint(10000))  # ê³ ìœ í•œ key ìƒì„±
selected_features = st.multiselect("ë³€ìˆ˜ ì„ íƒ", features, key=feature_selector_key)

# ì„ íƒí•œ ë³€ìˆ˜ë“¤ì— ëŒ€í•´ ìŠ¬ë¼ì´ë” ì„¤ì •
input_data = []
for feature in selected_features:
    min_val = float(df_2[feature].min())
    max_val = float(df_2[feature].max())
    slider_key = f"slider_{feature}_{str(np.random.randint(10000))}"  # ê³ ìœ í•œ key ìƒì„±
    value = st.slider(f"â­{feature} ê°’ì„ ì…ë ¥í•˜ì„¸ìš”:", min_value=min_val, max_value=max_val, value=(min_val + max_val) / 2, key=slider_key)

    input_data.append(value)

# ì…ë ¥ ë°ì´í„°ë¡œ ì˜ˆì¸¡ ì‹¤í–‰
predict_button_key = "predict_button_" + str(np.random.randint(10000))  # ê³ ìœ í•œ key ìƒì„±
if st.button("ğŸ›¬ì˜ˆì¸¡ ì‹¤í–‰", key=predict_button_key):
    if 'cat_2' in locals():
        input_scaled = np.array(input_data).reshape(1, -1)
        prediction_result = predict_fault(input_scaled[0], selected_features)
        st.write(prediction_result)
    else:
        st.write("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'ëª¨ë¸ í•™ìŠµ' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”.")
        
        
st.divider()

############################ Component 3A ############################
# ë°ì´í„° ë¡œë“œ
st.subheader("ğŸ›¢ï¸COMPONENT3A ëª¨ë¸ë§ ê²°ê³¼")
df = pd.read_csv("./data/casting.csv")

# ì»´í¬ë„ŒíŠ¸ ë¶„ë¦¬
df_3 = df[df['COMPONENT_ARBITRARY'] == 'COMPONENT3']

# ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ 1ì¸ ë³€ìˆ˜ë“¤ ëª©ë¡
columns_to_drop = ['FUEL', 'FOPTIMETHGLY', 'FH2O', 'FOXID', 'U100', 'U75', 'U50', 'U25', 'U20', 'U14', 'U6', 'U4', 'FSO4', 'V100', 'FTBN', 'SOOTPERCENTAGE', 'FNOX', 'ID', 'COMPONENT_ARBITRARY', 'YEAR', 'SAMPLE_TRANSFER_DAY']

# ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ 1ì¸ ë³€ìˆ˜ë“¤ ì œê±°
df_3 = df_3.drop(columns=columns_to_drop)

# ê²°ì¸¡ì¹˜ ëŒ€ì¹˜
cd_mode = df_3['CD'].mode()[0]
df_3['CD'].fillna(cd_mode, inplace=True)

# K ë³€ìˆ˜ ëª¨ë¸ë§ (ê²°ì¸¡ì¹˜ ëŒ€ì¹˜)
k_missing = df_3[df_3['K'].isnull()]
k_not_missing = df_3.dropna(subset=['K'])

# í”¼ì²˜ì™€ íƒ€ê²Ÿ ë³€ìˆ˜ ì„¤ì •
X = k_not_missing[['NA', 'SI']]
y = k_not_missing['K']

# ëª¨ë¸ í•™ìŠµ
model = LinearRegression()
model.fit(X, y)

# ê²°ì¸¡ì¹˜ë¥¼ ê°€ì§€ëŠ” ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡
X_missing = k_missing[['NA', 'SI']]
k_missing['K'] = model.predict(X_missing)

# ê²°ì¸¡ì¹˜ë¥¼ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„ì„ ì—…ë°ì´íŠ¸
df_3.update(k_missing)

# í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
kmeans = KMeans(n_clusters=3, random_state=0)
df_3['Cluster'] = kmeans.fit_predict(df_3[['S', 'P']])

# ê° í´ëŸ¬ìŠ¤í„°ë¥¼ ë³„ë„ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì €ì¥
df_3a = df_3[df_3['Cluster'] == 0]

# íŠ¹ì„±(Feature)ì™€ ë¼ë²¨(Label) ë¶„ë¦¬
Xa = df_3a.drop(columns=['Y_LABEL'])
ya = df_3a['Y_LABEL']

# Robust Scaler ì ìš©
scaler = RobustScaler()
X_scaleda = scaler.fit_transform(Xa)
X_scaled_df_3a = pd.DataFrame(X_scaleda, columns=Xa.columns) 

# ë…ë¦½ë³€ìˆ˜ ì¢…ì†ë³€ìˆ˜ ë¶„í• 
Xa = X_scaled_df_3a
ya = df_3a['Y_LABEL']

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 
X_a, X_vala, y_a, y_vala = train_test_split(Xa, ya, test_size=0.2, random_state=42)

# CatBoost ë¶„ë¥˜ ëª¨ë¸ ìƒì„± ë° í›ˆë ¨
cat_3a = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=4, verbose=0, border_count=32, l2_leaf_reg=5)
cat_3a.fit(X_a, y_a)

# ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
save_dir = 'models'
os.makedirs(save_dir, exist_ok=True)  # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±

# ëª¨ë¸ ì €ì¥
joblib.dump(cat_3a, os.path.join(save_dir, 'cat_3a_model.joblib'))

# ì˜ˆì¸¡ í™•ë¥  ê³„ì‚°
pred_ca_proba = cat_3a.predict_proba(X_vala)[:, 1]  # í´ë˜ìŠ¤ 1 í™•ë¥ 

# F1 Score ê³„ì‚°
pred_ca = (pred_ca_proba >= 0.5).astype(int)  # ì„ê³„ê°’ 0.5 ê¸°ì¤€ìœ¼ë¡œ í´ë˜ìŠ¤ ì˜ˆì¸¡
f1 = f1_score(y_vala, pred_ca)

print(f"F1 Score: {f1:.2f}")

# ë³€ìˆ˜ ì¤‘ìš”ë„ í™•ì¸
feature_importance = cat_3a.get_feature_importance()
important_features = np.argsort(feature_importance)[-3:]

print(f"ê°€ì¥ ì¤‘ìš”í•œ ë³€ìˆ˜: {Xa.columns[important_features]}")

# ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜
def predict_fault(input_values, selected_features):
    # ì „ì²´ ë°ì´í„°ì˜ ê¸¸ì´ë¥¼ ë§ì¶”ê¸° ìœ„í•´ 0ìœ¼ë¡œ ì´ˆê¸°í™”
    data = np.zeros((1, len(Xa.columns)))
    
    # ì„ íƒí•œ ë³€ìˆ˜ì— ëŒ€í•œ ê°’ í• ë‹¹
    for i, feature in enumerate(selected_features):
        data[0, Xa.columns.get_loc(feature)] = input_values[i]

    # ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ìŠ¤ì¼€ì¼ë§
    data_scaled = scaler.transform(data)
    prediction = cat_3a.predict_proba(data_scaled)[:, 1]
    if prediction[0] < 0.40:
        return f"ğŸŸ¢:ì •ìƒ: {prediction[0]:.2f} í™•ë¥ ë¡œ ë¶ˆëŸ‰ì…ë‹ˆë‹¤.ê³„ì† ì‚¬ìš©í•˜ì„¸ìš”."
    elif 0.40 <= prediction[0] < 0.45:
        return f"ğŸŸ¡ì£¼ì˜: {prediction[0]:.2f} í™•ë¥ ë¡œ ë¶ˆëŸ‰ì…ë‹ˆë‹¤.ì‚¬ìš©ì— ì£¼ì˜í•˜ì„¸ìš”."
    elif 0.45 <= prediction[0] < 0.50:
        return f"ğŸŸ ê²½ê³ : {prediction[0]:.2f} í™•ë¥ ë¡œ ë¶ˆëŸ‰ì…ë‹ˆë‹¤.ë§ˆìŒì˜ ì¤€ë¹„ê°€ í•„ìš”í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤."
    else:
        return f"ğŸ”´ìœ„í—˜: {prediction[0]:.2f} í™•ë¥ ë¡œ ë¶ˆëŸ‰ì…ë‹ˆë‹¤.ë‹¹ì¥ êµì²´í•˜ì„¸ìš”."

# ëª¨ë¸ í•™ìŠµ ë²„íŠ¼
if st.button("ğŸ›«ëª¨ë¸ í•™ìŠµ"):
    cat_3a.fit(X_a, y_a)
    st.write("ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# ë³€ìˆ˜ ì„ íƒ ë° ìŠ¬ë¼ì´ë” ì„¤ì •
features = list(Xa.columns)
selected_features = st.multiselect("â­ë³€ìˆ˜ ì„ íƒ", features)

# ì„ íƒí•œ ë³€ìˆ˜ë“¤ì— ëŒ€í•´ ìŠ¬ë¼ì´ë” ì„¤ì •
input_data = []
for feature in selected_features:
    min_val = float(df_3[feature].min())
    max_val = float(df_3[feature].max())
    value = st.slider(f"ğŸ’«{feature} ê°’ì„ ì…ë ¥í•˜ì„¸ìš”:", min_value=min_val, max_value=max_val, value=(min_val + max_val) / 2)
    input_data.append(value)

# ì…ë ¥ ë°ì´í„°ë¡œ ì˜ˆì¸¡ ì‹¤í–‰
if st.button("ì˜ˆì¸¡ ì‹¤í–‰"):
    if 'cat_3a' in locals():
        input_scaled = np.array(input_data).reshape(1, -1)
        prediction_result = predict_fault(input_scaled[0], selected_features)
        st.write(prediction_result)
    else:
        st.write("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'ëª¨ë¸ í•™ìŠµ' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”.")
        
        
st.divider()
############################ Component 3B ############################

# ë°ì´í„° ë¡œë“œ
st.subheader("ğŸ›¢ï¸COMPONENT3B ëª¨ë¸ë§ ê²°ê³¼")
df = pd.read_csv("./data/casting.csv")

# ì»´í¬ë„ŒíŠ¸ ë¶„ë¦¬
df_3 = df[df['COMPONENT_ARBITRARY'] == 'COMPONENT3']

# ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ 1ì¸ ë³€ìˆ˜ë“¤ ëª©ë¡
columns_to_drop = ['FUEL', 'FOPTIMETHGLY', 'FH2O', 'FOXID', 'U100', 'U75', 'U50', 'U25', 'U20', 'U14', 'U6', 'U4', 'FSO4', 'V100', 'FTBN', 'SOOTPERCENTAGE', 'FNOX', 'ID', 'COMPONENT_ARBITRARY', 'YEAR', 'SAMPLE_TRANSFER_DAY']

# ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ 1ì¸ ë³€ìˆ˜ë“¤ ì œê±°
df_3 = df_3.drop(columns=columns_to_drop)

# ê²°ì¸¡ì¹˜ ëŒ€ì¹˜
cd_mode = df_3['CD'].mode()[0]
df_3['CD'].fillna(cd_mode, inplace=True)

# K ë³€ìˆ˜ ëª¨ë¸ë§ (ê²°ì¸¡ì¹˜ ëŒ€ì¹˜)
k_missing = df_3[df_3['K'].isnull()]
k_not_missing = df_3.dropna(subset=['K'])

# í”¼ì²˜ì™€ íƒ€ê²Ÿ ë³€ìˆ˜ ì„¤ì •
X = k_not_missing[['NA', 'SI']]
y = k_not_missing['K']

# ëª¨ë¸ í•™ìŠµ
model = LinearRegression()
model.fit(X, y)

# ê²°ì¸¡ì¹˜ë¥¼ ê°€ì§€ëŠ” ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡
X_missing = k_missing[['NA', 'SI']]
k_missing['K'] = model.predict(X_missing)

# ê²°ì¸¡ì¹˜ë¥¼ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„ì„ ì—…ë°ì´íŠ¸
df_3.update(k_missing)

# í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
kmeans = KMeans(n_clusters=3, random_state=0)
df_3['Cluster'] = kmeans.fit_predict(df_3[['S', 'P']])

# ê° í´ëŸ¬ìŠ¤í„°ë¥¼ ë³„ë„ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì €ì¥
df_3b = df_3[df_3['Cluster'] == 1]

# íŠ¹ì„±(Feature)ì™€ ë¼ë²¨(Label) ë¶„ë¦¬
Xb = df_3b.drop(columns=['Y_LABEL'])
yb = df_3b['Y_LABEL']

# Robust Scaler ì ìš©
scaler = RobustScaler()
X_scaledb = scaler.fit_transform(Xb)
X_scaled_df_3b = pd.DataFrame(X_scaledb, columns=Xb.columns)

# ë…ë¦½ë³€ìˆ˜ ì¢…ì†ë³€ìˆ˜ ë¶„í• 
Xb = X_scaled_df_3b
yb = df_3b['Y_LABEL']

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 
X_b, X_valb, y_b, y_valb = train_test_split(Xb, yb, test_size=0.2, random_state=42)

# CatBoost ëª¨ë¸ ì„¤ì • ë° í•™ìŠµ
best_cat_3b = CatBoostClassifier(
    iterations=200,
    learning_rate=0.1,
    depth=4,
    border_count=50,
    loss_function='Logloss',
    eval_metric='F1',
    verbose=10
)

best_cat_3b.fit(X_b, y_b)

# ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
save_dir = 'models'
os.makedirs(save_dir, exist_ok=True)  # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±

# ëª¨ë¸ ì €ì¥
joblib.dump(best_cat_3b, os.path.join(save_dir, 'cat_3b_model.joblib'))

# í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ëŒ€í•´ ì˜ˆì¸¡
y_predb = best_cat_3b.predict(X_valb)

# F1 ìŠ¤ì½”ì–´ ê³„ì‚°
f1 = f1_score(y_valb, y_predb)
print("F1 Score of y_predb(Best): {:.2f}".format(f1))
# F1 Score: 0.71

# ëª¨ë¸ í•™ìŠµ ë²„íŠ¼
if st.button("ğŸ›«ëª¨ë¸ í•™ìŠµ", key="model_train_button"):
    best_cat_3b.fit(X_b, y_b)
    st.write("ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# ë³€ìˆ˜ ì„ íƒ ë° ìŠ¬ë¼ì´ë” ì„¤ì •
features = list(Xb.columns)
selected_features = st.multiselect("â­ë³€ìˆ˜ ì„ íƒ", features, key="feature_selector")

# ì„ íƒí•œ ë³€ìˆ˜ë“¤ì— ëŒ€í•´ ìŠ¬ë¼ì´ë” ì„¤ì •
input_data = []
for feature in selected_features:
    min_val = float(df_3b[feature].min())
    max_val = float(df_3b[feature].max())
    value = st.slider(f"ğŸ’«{feature} ê°’ì„ ì…ë ¥í•˜ì„¸ìš”:", min_value=min_val, max_value=max_val, value=(min_val + max_val) / 2, key=f"slider_{feature}")

# ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜
def predict_fault(input_values, selected_features):
    # ì „ì²´ ë°ì´í„°ì˜ ê¸¸ì´ë¥¼ ë§ì¶”ê¸° ìœ„í•´ 0ìœ¼ë¡œ ì´ˆê¸°í™”
    data = np.zeros((1, len(Xb.columns)))
    
    # ì„ íƒí•œ ë³€ìˆ˜ì— ëŒ€í•œ ê°’ í• ë‹¹
    for i, feature in enumerate(selected_features):
        data[0, Xb.columns.get_loc(feature)] = input_values[i]

    # ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ìŠ¤ì¼€ì¼ë§
    data_scaled = scaler.transform(data)
    prediction = best_cat_3b.predict_proba(data_scaled)[:, 1]
    if prediction[0] < 0.40:
        return f"ğŸŸ¢:ì •ìƒ: {prediction[0]:.2f} í™•ë¥ ë¡œ ë¶ˆëŸ‰ì…ë‹ˆë‹¤. ê³„ì† ì‚¬ìš©í•˜ì„¸ìš”."
    elif 0.40 <= prediction[0] < 0.45:
        return f"ğŸŸ¡ì£¼ì˜: {prediction[0]:.2f} í™•ë¥ ë¡œ ë¶ˆëŸ‰ì…ë‹ˆë‹¤. ì‚¬ìš©ì— ì£¼ì˜í•˜ì„¸ìš”."
    elif 0.45 <= prediction[0] < 0.50:
        return f"ğŸŸ ê²½ê³ : {prediction[0]:.2f} í™•ë¥ ë¡œ ë¶ˆëŸ‰ì…ë‹ˆë‹¤. ë§ˆìŒì˜ ì¤€ë¹„ê°€ í•„ìš”í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤."
    else:
        return f"ğŸ”´ìœ„í—˜: {prediction[0]:.2f} í™•ë¥ ë¡œ ë¶ˆëŸ‰ì…ë‹ˆë‹¤. ë‹¹ì¥ êµì²´í•˜ì„¸ìš”."

# ì…ë ¥ ë°ì´í„°ë¡œ ì˜ˆì¸¡ ì‹¤í–‰
if st.button("ğŸ›¬ì˜ˆì¸¡ ì‹¤í–‰", key="predict_button"):
    if 'best_cat_3b' in locals():
        input_scaled = np.array(input_data).reshape(1, -1)
        prediction_result = predict_fault(input_scaled[0], selected_features)
        st.write(prediction_result)
    else:
        st.write("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'ëª¨ë¸ í•™ìŠµ' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”.")
        
        
st.divider()

############################ Component 3C ############################
# ë°ì´í„° ë¡œë“œ
st.subheader("ğŸ›¢ï¸COMPONENT3C ëª¨ë¸ë§ ê²°ê³¼")
df = pd.read_csv("./data/casting.csv")

# ì»´í¬ë„ŒíŠ¸ ë¶„ë¦¬
df_3 = df[df['COMPONENT_ARBITRARY'] == 'COMPONENT3']

# ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ 1ì¸ ë³€ìˆ˜ë“¤ ëª©ë¡
columns_to_drop = ['FUEL', 'FOPTIMETHGLY', 'FH2O', 'FOXID', 'U100', 'U75', 'U50', 'U25', 'U20', 'U14', 'U6', 'U4', 'FSO4', 'V100', 'FTBN', 'SOOTPERCENTAGE', 'FNOX', 'ID', 'COMPONENT_ARBITRARY', 'YEAR', 'SAMPLE_TRANSFER_DAY']

# ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ 1ì¸ ë³€ìˆ˜ë“¤ ì œê±°
df_3 = df_3.drop(columns=columns_to_drop)

# ê²°ì¸¡ì¹˜ ëŒ€ì¹˜
cd_mode = df_3['CD'].mode()[0]
df_3['CD'].fillna(cd_mode, inplace=True)

# K ë³€ìˆ˜ ëª¨ë¸ë§ (ê²°ì¸¡ì¹˜ ëŒ€ì¹˜)
k_missing = df_3[df_3['K'].isnull()]
k_not_missing = df_3.dropna(subset=['K'])

# í”¼ì²˜ì™€ íƒ€ê²Ÿ ë³€ìˆ˜ ì„¤ì •
X = k_not_missing[['NA', 'SI']]
y = k_not_missing['K']

# ëª¨ë¸ í•™ìŠµ
model = LinearRegression()
model.fit(X, y)

# ê²°ì¸¡ì¹˜ë¥¼ ê°€ì§€ëŠ” ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡
X_missing = k_missing[['NA', 'SI']]
k_missing['K'] = model.predict(X_missing)

# ê²°ì¸¡ì¹˜ë¥¼ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„ì„ ì—…ë°ì´íŠ¸
df_3.update(k_missing)

# í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
kmeans = KMeans(n_clusters=3, random_state=0)
df_3['Cluster'] = kmeans.fit_predict(df_3[['S', 'P']])

# ê° í´ëŸ¬ìŠ¤í„°ë¥¼ ë³„ë„ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì €ì¥
df_3c = df_3[df_3['Cluster'] == 2]

# íŠ¹ì„±(Feature)ì™€ ë¼ë²¨(Label) ë¶„ë¦¬
Xc = df_3c.drop(columns=['Y_LABEL'])
yc = df_3c['Y_LABEL']

# Robust Scaler ì ìš©
scaler = RobustScaler()
X_scaledc = scaler.fit_transform(Xc)
X_scaled_df_3c = pd.DataFrame(X_scaledc, columns=Xc.columns)

# ë…ë¦½ë³€ìˆ˜ ì¢…ì†ë³€ìˆ˜ ë¶„í• 
Xc = X_scaled_df_3c
yc = df_3c['Y_LABEL']

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 
X_c, X_valc, y_c, y_valc = train_test_split(Xc, yc, test_size=0.2, random_state=42)

# CatBoost ëª¨ë¸ ì„¤ì • ë° í•™ìŠµ
best_cat_3c = CatBoostClassifier(
    iterations=500,
    learning_rate=0.05,
    depth=7,
    l2_leaf_reg=3,
    border_count=100,
    loss_function='Logloss',
    eval_metric='F1',
    verbose=10
)

best_cat_3c.fit(X_c, y_c)

# ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
save_dir = 'models'
os.makedirs(save_dir, exist_ok=True)  # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±

# ëª¨ë¸ ì €ì¥
joblib.dump(best_cat_3c, os.path.join(save_dir, 'cat_3c_model.joblib'))

# í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ëŒ€í•´ ì˜ˆì¸¡
y_predc = best_cat_3c.predict(X_valc)

# F1 ìŠ¤ì½”ì–´ ê³„ì‚°
f1 = f1_score(y_valc, y_predc)
print("F1 Score of y_predc(Best): {:.2f}".format(f1))
# F1 Score: 0.71

# Streamlit ëŒ€ì‹œë³´ë“œ
# ëª¨ë¸ í•™ìŠµ ë²„íŠ¼
if st.button("ğŸ›«ëª¨ë¸ í•™ìŠµ", key="model_train_button_c"):
    best_cat_3c.fit(X_c, y_c)
    st.write("ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# ë³€ìˆ˜ ì„ íƒ ë° ìŠ¬ë¼ì´ë” ì„¤ì •
features = list(Xc.columns)
selected_features = st.multiselect("â­ë³€ìˆ˜ ì„ íƒ", features, key="feature_selector_c")

# ì„ íƒí•œ ë³€ìˆ˜ë“¤ì— ëŒ€í•´ ìŠ¬ë¼ì´ë” ì„¤ì •
input_data = []
for feature in selected_features:
    min_val = float(df_3c[feature].min())
    max_val = float(df_3c[feature].max())
    value = st.slider(f"ğŸ’«{feature} ê°’ì„ ì…ë ¥í•˜ì„¸ìš”:", min_value=min_val, max_value=max_val, value=(min_val + max_val) / 2, key=f"slider_{feature}_c")

# ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜
def predict_fault(input_values, selected_features):
    # ì „ì²´ ë°ì´í„°ì˜ ê¸¸ì´ë¥¼ ë§ì¶”ê¸° ìœ„í•´ 0ìœ¼ë¡œ ì´ˆê¸°í™”
    data = np.zeros((1, len(Xc.columns)))
    
    # ì„ íƒí•œ ë³€ìˆ˜ì— ëŒ€í•œ ê°’ í• ë‹¹
    for i, feature in enumerate(selected_features):
        data[0, Xc.columns.get_loc(feature)] = input_values[i]

    # ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ìŠ¤ì¼€ì¼ë§
    data_scaled = scaler.transform(data)
    prediction = best_cat_3c.predict_proba(data_scaled)[:, 1]
    if prediction[0] < 0.40:
        return f"ğŸŸ¢:ì •ìƒ: {prediction[0]:.2f} í™•ë¥ ë¡œ ë¶ˆëŸ‰ì…ë‹ˆë‹¤. ê³„ì† ì‚¬ìš©í•˜ì„¸ìš”."
    elif 0.40 <= prediction[0] < 0.45:
        return f"ğŸŸ¡ì£¼ì˜: {prediction[0]:.2f} í™•ë¥ ë¡œ ë¶ˆëŸ‰ì…ë‹ˆë‹¤. ì‚¬ìš©ì— ì£¼ì˜í•˜ì„¸ìš”."
    elif 0.45 <= prediction[0] < 0.50:
        return f"ğŸŸ ê²½ê³ : {prediction[0]:.2f} í™•ë¥ ë¡œ ë¶ˆëŸ‰ì…ë‹ˆë‹¤. ë§ˆìŒì˜ ì¤€ë¹„ê°€ í•„ìš”í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤."
    else:
        return f"ğŸ”´ìœ„í—˜: {prediction[0]:.2f} í™•ë¥ ë¡œ ë¶ˆëŸ‰ì…ë‹ˆë‹¤. ë‹¹ì¥ êµì²´í•˜ì„¸ìš”."

# ì…ë ¥ ë°ì´í„°ë¡œ ì˜ˆì¸¡ ì‹¤í–‰
if st.button("ğŸ›¬ì˜ˆì¸¡ ì‹¤í–‰", key="predict_button_c"):
    if 'best_cat_3c' in locals():
        input_scaled = np.array(input_data).reshape(1, -1)
        prediction_result = predict_fault(input_scaled[0], selected_features)
        st.write(prediction_result)
    else:
        st.write("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'ëª¨ë¸ í•™ìŠµ' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”.")
        
        
st.divider()

############################ Component ALL & 4 ############################       
        
# ë°ì´í„° ë¡œë“œ
st.subheader("ğŸ›¢ï¸COMPONENT ALL (4) ëª¨ë¸ë§ ê²°ê³¼")
df = pd.read_csv("./data/casting.csv")

# ì œê±°í•  ì—´ ëª©ë¡
columns_to_drop = ['ID', 'YEAR', 'SAMPLE_TRANSFER_DAY', 'COMPONENT_ARBITRARY', 'U100', 'U75', 'U50', 'U25', 'U14', 'U6', 'U4', 'FH2O', 'FNOX', 'FOPTIMETHGLY', 'FOXID', 'FSO4', 'FTBN', 'FUEL', 'SOOTPERCENTAGE']

# ì—´ ì œê±°
df = df.drop(columns=columns_to_drop)

# CD ì—´ì˜ ê²°ì¸¡ì¹˜ë¥¼ 0ìœ¼ë¡œ ëŒ€ì¹˜
df['CD'] = df['CD'].fillna(df['CD'].mode()[0])

# K ë³€ìˆ˜ì˜ ê²°ì¸¡ì¹˜ë¥¼ ê°€ì§€ëŠ” í–‰ë§Œ ì„ íƒ
k_missing = df[df['K'].isnull()]

# ê²°ì¸¡ì¹˜ë¥¼ ê°€ì§€ì§€ ì•ŠëŠ” K ë³€ìˆ˜ì™€ ê´€ë ¨ ë³€ìˆ˜ë“¤ ì„ íƒ
k_not_missing = df.dropna(subset=['K'])

# í”¼ì²˜ì™€ íƒ€ê²Ÿ ë³€ìˆ˜ ì„¤ì •
X = k_not_missing[['NA', 'SI']]
y = k_not_missing['K']

# ëª¨ë¸ í•™ìŠµ
model = LinearRegression()
model.fit(X, y)

# ê²°ì¸¡ì¹˜ë¥¼ ê°€ì§€ëŠ” ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡
X_missing = k_missing[['NA', 'SI']]
k_missing['K'] = model.predict(X_missing)

# ê²°ì¸¡ì¹˜ë¥¼ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„ì„ ì—…ë°ì´íŠ¸
df.update(k_missing)

# ë…ë¦½ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ ë¶„í• 
X = df.drop(columns=['Y_LABEL'])
y = df['Y_LABEL']

# Robust Scaler ì ìš©
scaler = RobustScaler()
X_scaled = scaler.fit_transform(X)

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 
X_tr, X_val, y_tr, y_val = train_test_split(X_scaled, y, test_size=0.1, random_state=42)

# CatBoost ëª¨ë¸ ì„¤ì • ë° í•™ìŠµ
best_cat = CatBoostClassifier(
    iterations=100,
    learning_rate=0.2,
    depth=4,
    l2_leaf_reg=3,
    border_count=32,
    verbose=0
)

best_cat.fit(X_tr, y_tr)

# ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
save_dir = 'models'
os.makedirs(save_dir, exist_ok=True)  # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±

# ëª¨ë¸ ì €ì¥
joblib.dump(best_cat, os.path.join(save_dir, 'cat_4_model.joblib'))

# í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ëŒ€í•´ ì˜ˆì¸¡
y_pred = best_cat.predict(X_val)

# F1 ìŠ¤ì½”ì–´ ê³„ì‚°
f1 = f1_score(y_val, y_pred, average='macro')
print("F1 Score: {:.2f}".format(f1))

# ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜
def predict_fault(input_values, selected_features):
    data = np.zeros((1, X_scaled.shape[1]))
    for i, feature in enumerate(selected_features):
        data[0, list(df.columns).index(feature)] = input_values[i]
    data_scaled = scaler.transform(data)
    prediction = best_cat.predict_proba(data_scaled)[:, 1]
    if prediction[0] < 0.40:
        return f"ğŸŸ¢:ì •ìƒ: {prediction[0]:.2f} í™•ë¥ ë¡œ ë¶ˆëŸ‰ì…ë‹ˆë‹¤. ê³„ì† ì‚¬ìš©í•˜ì„¸ìš”."
    elif 0.40 <= prediction[0] < 0.45:
        return f"ğŸŸ¡ì£¼ì˜: {prediction[0]:.2f} í™•ë¥ ë¡œ ë¶ˆëŸ‰ì…ë‹ˆë‹¤. ì‚¬ìš©ì— ì£¼ì˜í•˜ì„¸ìš”."
    elif 0.45 <= prediction[0] < 0.50:
        return f"ğŸŸ ê²½ê³ : {prediction[0]:.2f} í™•ë¥ ë¡œ ë¶ˆëŸ‰ì…ë‹ˆë‹¤. ë§ˆìŒì˜ ì¤€ë¹„ê°€ í•„ìš”í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤."
    else:
        return f"ğŸ”´ìœ„í—˜: {prediction[0]:.2f} í™•ë¥ ë¡œ ë¶ˆëŸ‰ì…ë‹ˆë‹¤. ë‹¹ì¥ êµì²´í•˜ì„¸ìš”."

# ëª¨ë¸ í•™ìŠµ ë²„íŠ¼
train_button_key = "train_model_button_" + str(np.random.randint(10000))  # ê³ ìœ í•œ key ìƒì„±
if st.button("ğŸ›«ëª¨ë¸ í•™ìŠµ", key=train_button_key):
    best_cat.fit(X_tr, y_tr)
    st.write("ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# ë³€ìˆ˜ ì„ íƒ ë° ìŠ¬ë¼ì´ë” ì„¤ì •
features = list(df.columns)
feature_selector_key = "feature_selector_" + str(np.random.randint(10000))  # ê³ ìœ í•œ key ìƒì„±
selected_features = st.multiselect("â­ë³€ìˆ˜ ì„ íƒ", features, key=feature_selector_key)

# ì„ íƒí•œ ë³€ìˆ˜ë“¤ì— ëŒ€í•´ ìŠ¬ë¼ì´ë” ì„¤ì •
input_data = []
for feature in selected_features:
    min_val = float(df[feature].min())
    max_val = float(df[feature].max())
    slider_key = f"slider_{feature}_{str(np.random.randint(10000))}"  # ê³ ìœ í•œ key ìƒì„±
    value = st.slider(f"ğŸ’«{feature} ê°’ì„ ì…ë ¥í•˜ì„¸ìš”:", min_value=min_val, max_value=max_val, value=(min_val + max_val) / 2, key=slider_key)

    input_data.append(value)

# ì…ë ¥ ë°ì´í„°ë¡œ ì˜ˆì¸¡ ì‹¤í–‰
predict_button_key = "predict_button_" + str(np.random.randint(10000))  # ê³ ìœ í•œ key ìƒì„±
if st.button("ğŸ›¬ì˜ˆì¸¡ ì‹¤í–‰", key=predict_button_key):
    if 'best_cat' in locals():
        input_scaled = np.array(input_data).reshape(1, -1)
        prediction_result = predict_fault(input_scaled[0], selected_features)
        st.write(prediction_result)
    else:
        st.write("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'ëª¨ë¸ í•™ìŠµ' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”.")
